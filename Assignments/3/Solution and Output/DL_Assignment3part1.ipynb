{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment3part1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewB7JnYBR1Ef",
        "colab_type": "text"
      },
      "source": [
        "# Defining model units and number of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEyFQUFLJBcN",
        "colab_type": "code",
        "outputId": "67276af8-3540-44c5-dabe-a27010a689a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "#print(tf.__version__)\n",
        "n_inputs = 28*28 \n",
        "n_hidden1 = 1024 \n",
        "n_hidden2 = 1024\n",
        "n_hidden3 = 1024\n",
        "n_hidden4 = 1024\n",
        "n_hidden5 = 1024\n",
        "n_outputs = 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWh4Ch5tJIhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvCF0ISxSGBe",
        "colab_type": "text"
      },
      "source": [
        "# Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR2YwBOAJL8d",
        "colab_type": "code",
        "outputId": "1bcb4b65-219e-4bff-a05e-d7d4fad961da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from tensorflow.contrib.layers import fully_connected\n",
        "with tf.variable_scope(\"dnn\"):\n",
        "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\") \n",
        "    hidden2 = fully_connected(hidden1, n_hidden2,scope=\"hidden2\")\n",
        "    hidden3 = fully_connected(hidden2, n_hidden3,scope=\"hidden3\")\n",
        "    hidden4 = fully_connected(hidden3, n_hidden4,scope=\"hidden4\")\n",
        "    hidden5 = fully_connected(hidden4, n_hidden5,scope=\"hidden5\")\n",
        "    logits = fully_connected(hidden5, n_outputs, scope=\"outputs\",activation_fn=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEgHS0l8SMy-",
        "colab_type": "text"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HODnXUIZJO2W",
        "colab_type": "code",
        "outputId": "574173b1-aa3c-4ae7-8328-31f4020849ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "        xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "        loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-d560affc419c>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK4PQsmGSQM3",
        "colab_type": "text"
      },
      "source": [
        "# Learning rate and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J99Dh04mJR3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) \n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezUONtvkSWQJ",
        "colab_type": "text"
      },
      "source": [
        "# Accuracy Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNRZhfKLJUct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct_pred = tf.equal(tf.math.argmax(logits,1),tf.math.argmax(y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrbStBw3Sehz",
        "colab_type": "text"
      },
      "source": [
        "# Reading MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbqvpNqSJXU1",
        "colab_type": "code",
        "outputId": "1778221f-5b25-4a04-ccb7-c7baf1201b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data \n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-b50643944b8d>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmFEpo4FSjbd",
        "colab_type": "text"
      },
      "source": [
        "# Epochs and batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiOVKxGkJZkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 40\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO9KwovbSniO",
        "colab_type": "text"
      },
      "source": [
        "# Training the model(base line network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX05zmA2JcXy",
        "colab_type": "code",
        "outputId": "db095edf-21af-4880-cd2a-88393d794eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "with tf.Session() as sess: \n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        print('Epoch',epoch, \"Train accuracy:\", acc_train)\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Train accuracy: 0.96\n",
            "Epoch 1 Train accuracy: 1.0\n",
            "Epoch 2 Train accuracy: 0.96\n",
            "Epoch 3 Train accuracy: 0.98\n",
            "Epoch 4 Train accuracy: 0.94\n",
            "Epoch 5 Train accuracy: 0.98\n",
            "Epoch 6 Train accuracy: 0.98\n",
            "Epoch 7 Train accuracy: 0.96\n",
            "Epoch 8 Train accuracy: 1.0\n",
            "Epoch 9 Train accuracy: 1.0\n",
            "Epoch 10 Train accuracy: 1.0\n",
            "Epoch 11 Train accuracy: 1.0\n",
            "Epoch 12 Train accuracy: 1.0\n",
            "Epoch 13 Train accuracy: 1.0\n",
            "Epoch 14 Train accuracy: 1.0\n",
            "Epoch 15 Train accuracy: 1.0\n",
            "Epoch 16 Train accuracy: 1.0\n",
            "Epoch 17 Train accuracy: 1.0\n",
            "Epoch 18 Train accuracy: 1.0\n",
            "Epoch 19 Train accuracy: 1.0\n",
            "Epoch 20 Train accuracy: 1.0\n",
            "Epoch 21 Train accuracy: 1.0\n",
            "Epoch 22 Train accuracy: 1.0\n",
            "Epoch 23 Train accuracy: 1.0\n",
            "Epoch 24 Train accuracy: 1.0\n",
            "Epoch 25 Train accuracy: 1.0\n",
            "Epoch 26 Train accuracy: 1.0\n",
            "Epoch 27 Train accuracy: 1.0\n",
            "Epoch 28 Train accuracy: 1.0\n",
            "Epoch 29 Train accuracy: 1.0\n",
            "Epoch 30 Train accuracy: 1.0\n",
            "Epoch 31 Train accuracy: 1.0\n",
            "Epoch 32 Train accuracy: 1.0\n",
            "Epoch 33 Train accuracy: 1.0\n",
            "Epoch 34 Train accuracy: 1.0\n",
            "Epoch 35 Train accuracy: 1.0\n",
            "Epoch 36 Train accuracy: 1.0\n",
            "Epoch 37 Train accuracy: 1.0\n",
            "Epoch 38 Train accuracy: 1.0\n",
            "Epoch 39 Train accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p8Ebz02Stg9",
        "colab_type": "text"
      },
      "source": [
        "# Restoring the model checkpoint and running on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nxss-w7Jqdr",
        "colab_type": "code",
        "outputId": "1eb75563-16a5-479d-e2be-1c87e620f0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "    for epoch in range(n_epochs): \n",
        "      acc_test1 = accuracy.eval(feed_dict={X: mnist.test.images[:1000],y: mnist.test.labels[:1000]})\n",
        "    print(\"Epoch:\",epoch, \"Test accuracy1:\", acc_test1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "Epoch: 39 Test accuracy1: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnshoNfZS0x7",
        "colab_type": "text"
      },
      "source": [
        "# Collecting weights and biases for all 6 layers(including output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI3K7CzGB5Hr",
        "colab_type": "code",
        "outputId": "acf83c1f-dfdd-4364-930b-3f40b77c3892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "    all_vars= tf.global_variables()\n",
        "    def var_name(name):\n",
        "        for i in range(len(all_vars)):\n",
        "            #print (all_vars)\n",
        "            #print(all_vars[i]) \n",
        "            if all_vars[i].name.startswith(name):\n",
        "                return all_vars[i]\n",
        "        return None\n",
        "    w1 = var_name('dnn/hidden1/weights')\n",
        "    w2 = var_name('dnn/hidden2/weights')\n",
        "    w3 = var_name('dnn/hidden3/weights')\n",
        "    w4 = var_name('dnn/hidden4/weights')\n",
        "    w5 = var_name('dnn/hidden5/weights')\n",
        "    w6 = var_name('dnn/outputs/weights')\n",
        "    b1=var_name('dnn/hidden1/biases')\n",
        "    b2=var_name('dnn/hidden1/biases')\n",
        "    b3=var_name('dnn/hidden1/biases')\n",
        "    b4=var_name('dnn/hidden1/biases')\n",
        "    b5=var_name('dnn/hidden1/biases')\n",
        "    b6=var_name('dnn/outputs/biases')\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWoSU8jId-sO",
        "colab_type": "code",
        "outputId": "5511ed61-66c0-46aa-8f1b-d7edeacdb46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "w1,w2,w3,w4,w5,w6,b1,b2,b3,b4,b5,b6"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Variable 'dnn/hidden1/weights:0' shape=(784, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden2/weights:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden3/weights:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden4/weights:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden5/weights:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/outputs/weights:0' shape=(1024, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden1/biases:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden1/biases:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden1/biases:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden1/biases:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/hidden1/biases:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'dnn/outputs/biases:0' shape=(10,) dtype=float32_ref>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO36lrCXCJ1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "w=[]\n",
        "b=[]\n",
        "w.append(w1)\n",
        "w.append(w2)\n",
        "w.append(w3)\n",
        "w.append(w4)\n",
        "w.append(w5)\n",
        "w.append(w6)\n",
        "b.append(b1)\n",
        "b.append(b2)\n",
        "b.append(b3)\n",
        "b.append(b4)\n",
        "b.append(b5)\n",
        "b.append(b6)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yKvXZ0XTw_-",
        "colab_type": "text"
      },
      "source": [
        "# Function to calculate svd and wbar\n",
        "1. We will get all singular values then we will convert the s to diagonal matrix named s_diag, and multiply with v to get vbar\n",
        "2. We will get approx weight(new weight) by multiplying vbar and u(left singular matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx1zi73GB1bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_svd(X):\n",
        "  s, u, v = tf.svd(X)\n",
        "  \n",
        "  s_diag = tf.linalg.diag(s)\n",
        "  \n",
        "  return s_diag, u, v\n",
        "\n",
        "def calculate_W_bar(s, u, v):\n",
        "  v_bar = tf.matmul(s, v, adjoint_b=True)\n",
        "  result = tf.matmul(u, v_bar)\n",
        "  \n",
        "  return result , v_bar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV-mPRTWU52t",
        "colab_type": "text"
      },
      "source": [
        "# Creating s1,u1,v1 matrix for all the weights except the last layer weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMVy4qpMB9-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1 = []\n",
        "u1 = []\n",
        "v1 = []\n",
        "for i in range(0 , len(w)-1):\n",
        "  s, u, v = calculate_svd(w[i])\n",
        "  \n",
        "  s1.append(s)\n",
        "  u1.append(u)\n",
        "  v1.append(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRDcPMo-C55x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = [10, 20, 50, 100, 200, 'DFull']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcfrOvdiVN4D",
        "colab_type": "text"
      },
      "source": [
        "# Creating W_bar list for approx weights for all 5 layers for each D value\n",
        "It contains 30 weight values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VElq-xFDURm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_bar_l = []\n",
        "s_d_list = []\n",
        "u_d_list = []\n",
        "v_d_list = []\n",
        "v_bar_list = []\n",
        "\n",
        "for d_val in D:\n",
        "  for j in range(0, len(w)-1):\n",
        "    if d_val == 'DFull':\n",
        "      s_d = s1[j][: , :]\n",
        "      u_d = u1[j][: , :]\n",
        "      v_d = v1[j][: , :]\n",
        "    else:\n",
        "      s_d = s1[j][:d_val , :d_val]\n",
        "      u_d = u1[j][: , :d_val]\n",
        "      v_d = v1[j][: , :d_val]\n",
        "        \n",
        "    s_d_list.append(s_d)\n",
        "    u_d_list.append(u_d)\n",
        "    v_d_list.append(v_d)\n",
        "    \n",
        "    result , v_bar = calculate_W_bar(s_d , u_d , v_d)\n",
        "    \n",
        "    W_bar_l.append(result)\n",
        "    v_bar_list.append(v_bar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgEnzeBCN-ny",
        "colab_type": "code",
        "outputId": "af2dfdd7-4b4e-4aab-ab47-4333e70e1bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(W_bar_l),len(v_bar_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxQR4yb65tMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sess.run(tf.assign(hidden1,W_bar_l[0]))\n",
        "#sess.run(tf.assign(hidden2,W_bar_l[1]))\n",
        "#sess.run(tf.assign(hidden3,W_bar_l[2]))\n",
        "#sess.run(tf.assign(hidden4,W_bar_l[3]))\n",
        "#sess.run(tf.assign(hidden5,W_bar_l[4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTEgsOURVh9V",
        "colab_type": "text"
      },
      "source": [
        "# Creating feed forward network to pass the newly created weights for each D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNvm66SXDh0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feed_forward(input , activation_layer , weights , biases , last_layer_weight):\n",
        "  result = []\n",
        "  n = len(weights)\n",
        "  for i in range(0 , n+1):\n",
        "    if i == 0:\n",
        "      temp = activation_layer[i](tf.matmul(input , weights[i]) + biases[i])\n",
        "    elif i < n:\n",
        "      temp = activation_layer[i](tf.matmul(result[i-1] , weights[i]) + biases[i])\n",
        "    else:\n",
        "      temp = activation_layer[i](tf.matmul(result[i-1] , last_layer_weight) + biases[i])\n",
        "    \n",
        "    result.append(temp)\n",
        "   \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7jQQ1adV-2B",
        "colab_type": "text"
      },
      "source": [
        "# Function to calculate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31mjGVXCD5B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_accuracy(x , y):\n",
        "  count_equal_values = tf.equal(tf.argmax(x, 1), tf.argmax(y, 1))\n",
        "  answer = count_equal_values.eval()\n",
        "  \n",
        "  count = 0\n",
        "  for i in range(len(answer)):\n",
        "    if answer[i] == True:\n",
        "      count+=1      \n",
        "  \n",
        "  return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mas8BoXrE4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#len(act_layers),len(W_bar_l),len(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMngS-S_7YA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x , test_y = mnist.test.next_batch(10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwwiqLpzWLWI",
        "colab_type": "text"
      },
      "source": [
        "# Restoring my model parameters and passing the new weights to calculate accuracy\n",
        "\n",
        "We can see the accuracy increases for higher value of D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxmHjEbMD6dQ",
        "colab_type": "code",
        "outputId": "e7d753a4-0eeb-4264-d866-8f1e4376187e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\n",
        "act_layers = [tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu,tf.nn.softmax]\n",
        "accuracy_list = []\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "    for i in range(0 , len(W_bar_l) , 5):\n",
        "      print(i)\n",
        "      ff_output = feed_forward(test_x , act_layers , W_bar_l[i : i+5] , b , w[5])\n",
        "      #print(ff_output)\n",
        "      accuracy = calculate_accuracy(ff_output[-1] , test_y)\n",
        "      accuracy = accuracy/test_y.shape[0]\n",
        "      \n",
        "      accuracy_list.append(accuracy)\n",
        "      print('Accuracy for D: ' + str(D[int(i/5)]) + ' is '+ str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0\n",
            "Accuracy for D: 10 is 0.4121\n",
            "5\n",
            "Accuracy for D: 20 is 0.513\n",
            "10\n",
            "Accuracy for D: 50 is 0.575\n",
            "15\n",
            "Accuracy for D: 100 is 0.707\n",
            "20\n",
            "Accuracy for D: 200 is 0.9317\n",
            "25\n",
            "Accuracy for D: DFull is 0.9793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4orqVi65Euc",
        "colab_type": "code",
        "outputId": "9ff0471b-137b-4e7d-af8b-ea3ab3e65684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(D , accuracy_list)\n",
        "plt.xlabel('D Singular Values')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSgQiIklOiUBEJMkpEYiIJLn/Bw3Aig4H80kyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl-K5HsQWpv1",
        "colab_type": "text"
      },
      "source": [
        "# Defining new model to increase performance of the old model for D=20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoRI7PORcxb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getModel(x , act_layers , neurons , u_weights , v_bar_weights , last_layer_weight):\n",
        "    num_layers = len(act_layers)\n",
        "    layers = [0]*num_layers\n",
        "    \n",
        "    if u_weights and v_bar_weights:\n",
        "      for i in range(0 , num_layers):        \n",
        "        if i == 0:\n",
        "            u_index = i // 2\n",
        "            kernel_weight = tf.constant_initializer(u_weights[u_index].eval())\n",
        "            layers[i] = tf.layers.dense(x , units= neurons[i] , activation=act_layers[i] , kernel_initializer=kernel_weight)   \n",
        "            \n",
        "        elif i < num_layers-1:\n",
        "          if i % 2 == 0:\n",
        "            u_index = i // 2\n",
        "            kernel_weight = tf.constant_initializer(u_weights[u_index].eval())\n",
        "            layers[i] = tf.layers.dense(layers[i-1] , units= neurons[i] , activation=act_layers[i] , kernel_initializer=kernel_weight)\n",
        "            \n",
        "          else:\n",
        "            v_index = i // 2\n",
        "            kernel_weight = tf.constant_initializer(v_bar_weights[v_index].eval())\n",
        "            layers[i] = tf.layers.dense(layers[i-1] , units= neurons[i] , activation=act_layers[i] , kernel_initializer=kernel_weight)\n",
        "            \n",
        "        else:\n",
        "            kernel_weight = tf.constant_initializer(last_layer_weight.eval())\n",
        "            layers[i] = tf.layers.dense(layers[i-1] , units= neurons[i] , activation=act_layers[i] , kernel_initializer=kernel_weight)\n",
        "    else:\n",
        "      for i in range(0 , num_layers):        \n",
        "          if i == 0:\n",
        "              layers[i] = tf.layers.dense(x , units= neurons[i] , activation=act_layers[i])        \n",
        "          elif i < num_layers-1:\n",
        "              layers[i] = tf.layers.dense(layers[i-1] , units= neurons[i] , activation=act_layers[i])\n",
        "          else:\n",
        "              layers[i] = tf.layers.dense(layers[i-1] , units= neurons[i] , activation=act_layers[i])\n",
        "    \n",
        "    return layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofBOKbxyXhc7",
        "colab_type": "text"
      },
      "source": [
        "# Defining Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PqNbg3i5E-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "new_learning_rate = 0.0003\n",
        "new_act_layers = [tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.softmax]\n",
        "new_neurons = [20, 1024, 20, 1024, 20, 1024, 20, 1024, 20, 1024, 10]\n",
        "new_num_layers = len(new_act_layers)\n",
        "new_batch_size = 1000\n",
        "total_train_images = np.shape(mnist.train.images)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIGnNSxOXuob",
        "colab_type": "text"
      },
      "source": [
        "# Passing the U values and Vbar values for D=20 to get apprimated weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQcJ_QeQcyuL",
        "colab_type": "code",
        "outputId": "e4bebedd-231a-4b56-c51e-6e6055f1ef71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    new_output = getModel(X, new_act_layers, new_neurons , u_d_list[5 : 10] , v_bar_list[5 : 10] , w[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-26-763c3d1456a9>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ1MwDbzYB69",
        "colab_type": "text"
      },
      "source": [
        "# Loss function,optimizer and accuracy formulas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMe4yajc_rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = new_output[new_num_layers - 1], labels = y)\n",
        "new_train_step = tf.train.AdamOptimizer(new_learning_rate).minimize(new_loss)\n",
        "new_correct_prediction = tf.equal(tf.argmax(new_output[new_num_layers - 1], 1), tf.argmax(y, 1))\n",
        "new_accuracy = tf.reduce_mean(tf.cast(new_correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RoLvAsgdDlb",
        "colab_type": "code",
        "outputId": "3e7e46b6-fbcf-4874-cebe-8445eda446c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "n_epochs=200\n",
        "for epoch in range(n_epochs):\n",
        "  for i in range(total_train_images // new_batch_size):      \n",
        "      \n",
        "    input_batch, labels_batch = mnist.train.next_batch(new_batch_size)\n",
        "    feed_dict = {X: input_batch, y: labels_batch}\n",
        "\n",
        "    new_train_step.run(feed_dict=feed_dict)\n",
        "    \n",
        "\n",
        "  test_x , test_y = mnist.test.next_batch(10000)\n",
        "  new_test_accuracy = new_accuracy.eval(feed_dict={X: test_x, y: test_y})\n",
        "  print(\"Epoch %d, testing accuracy %g\"%(epoch, new_test_accuracy))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, testing accuracy 0.3365\n",
            "Epoch 1, testing accuracy 0.497\n",
            "Epoch 2, testing accuracy 0.626\n",
            "Epoch 3, testing accuracy 0.7433\n",
            "Epoch 4, testing accuracy 0.7793\n",
            "Epoch 5, testing accuracy 0.7897\n",
            "Epoch 6, testing accuracy 0.7972\n",
            "Epoch 7, testing accuracy 0.8088\n",
            "Epoch 8, testing accuracy 0.8161\n",
            "Epoch 9, testing accuracy 0.8201\n",
            "Epoch 10, testing accuracy 0.8253\n",
            "Epoch 11, testing accuracy 0.8282\n",
            "Epoch 12, testing accuracy 0.8339\n",
            "Epoch 13, testing accuracy 0.8368\n",
            "Epoch 14, testing accuracy 0.8382\n",
            "Epoch 15, testing accuracy 0.8399\n",
            "Epoch 16, testing accuracy 0.8405\n",
            "Epoch 17, testing accuracy 0.8428\n",
            "Epoch 18, testing accuracy 0.8445\n",
            "Epoch 19, testing accuracy 0.8458\n",
            "Epoch 20, testing accuracy 0.8474\n",
            "Epoch 21, testing accuracy 0.848\n",
            "Epoch 22, testing accuracy 0.848\n",
            "Epoch 23, testing accuracy 0.8509\n",
            "Epoch 24, testing accuracy 0.849\n",
            "Epoch 25, testing accuracy 0.8505\n",
            "Epoch 26, testing accuracy 0.848\n",
            "Epoch 27, testing accuracy 0.8506\n",
            "Epoch 28, testing accuracy 0.854\n",
            "Epoch 29, testing accuracy 0.8539\n",
            "Epoch 30, testing accuracy 0.8544\n",
            "Epoch 31, testing accuracy 0.8521\n",
            "Epoch 32, testing accuracy 0.8511\n",
            "Epoch 33, testing accuracy 0.8544\n",
            "Epoch 34, testing accuracy 0.8563\n",
            "Epoch 35, testing accuracy 0.8547\n",
            "Epoch 36, testing accuracy 0.8536\n",
            "Epoch 37, testing accuracy 0.8574\n",
            "Epoch 38, testing accuracy 0.8558\n",
            "Epoch 39, testing accuracy 0.857\n",
            "Epoch 40, testing accuracy 0.8581\n",
            "Epoch 41, testing accuracy 0.8588\n",
            "Epoch 42, testing accuracy 0.86\n",
            "Epoch 43, testing accuracy 0.8596\n",
            "Epoch 44, testing accuracy 0.86\n",
            "Epoch 45, testing accuracy 0.8597\n",
            "Epoch 46, testing accuracy 0.8593\n",
            "Epoch 47, testing accuracy 0.863\n",
            "Epoch 48, testing accuracy 0.8618\n",
            "Epoch 49, testing accuracy 0.8637\n",
            "Epoch 50, testing accuracy 0.8633\n",
            "Epoch 51, testing accuracy 0.8736\n",
            "Epoch 52, testing accuracy 0.9171\n",
            "Epoch 53, testing accuracy 0.9253\n",
            "Epoch 54, testing accuracy 0.9286\n",
            "Epoch 55, testing accuracy 0.9255\n",
            "Epoch 56, testing accuracy 0.9325\n",
            "Epoch 57, testing accuracy 0.9335\n",
            "Epoch 58, testing accuracy 0.9356\n",
            "Epoch 59, testing accuracy 0.9376\n",
            "Epoch 60, testing accuracy 0.9356\n",
            "Epoch 61, testing accuracy 0.9339\n",
            "Epoch 62, testing accuracy 0.9365\n",
            "Epoch 63, testing accuracy 0.9394\n",
            "Epoch 64, testing accuracy 0.9381\n",
            "Epoch 65, testing accuracy 0.9378\n",
            "Epoch 66, testing accuracy 0.9426\n",
            "Epoch 67, testing accuracy 0.9437\n",
            "Epoch 68, testing accuracy 0.9437\n",
            "Epoch 69, testing accuracy 0.9429\n",
            "Epoch 70, testing accuracy 0.9446\n",
            "Epoch 71, testing accuracy 0.9449\n",
            "Epoch 72, testing accuracy 0.9444\n",
            "Epoch 73, testing accuracy 0.9441\n",
            "Epoch 74, testing accuracy 0.9426\n",
            "Epoch 75, testing accuracy 0.9446\n",
            "Epoch 76, testing accuracy 0.9377\n",
            "Epoch 77, testing accuracy 0.9464\n",
            "Epoch 78, testing accuracy 0.9431\n",
            "Epoch 79, testing accuracy 0.9467\n",
            "Epoch 80, testing accuracy 0.9465\n",
            "Epoch 81, testing accuracy 0.9499\n",
            "Epoch 82, testing accuracy 0.9469\n",
            "Epoch 83, testing accuracy 0.9456\n",
            "Epoch 84, testing accuracy 0.9474\n",
            "Epoch 85, testing accuracy 0.9483\n",
            "Epoch 86, testing accuracy 0.952\n",
            "Epoch 87, testing accuracy 0.9499\n",
            "Epoch 88, testing accuracy 0.948\n",
            "Epoch 89, testing accuracy 0.9475\n",
            "Epoch 90, testing accuracy 0.9391\n",
            "Epoch 91, testing accuracy 0.9498\n",
            "Epoch 92, testing accuracy 0.9517\n",
            "Epoch 93, testing accuracy 0.9512\n",
            "Epoch 94, testing accuracy 0.9508\n",
            "Epoch 95, testing accuracy 0.9543\n",
            "Epoch 96, testing accuracy 0.9487\n",
            "Epoch 97, testing accuracy 0.9464\n",
            "Epoch 98, testing accuracy 0.9452\n",
            "Epoch 99, testing accuracy 0.9517\n",
            "Epoch 100, testing accuracy 0.9505\n",
            "Epoch 101, testing accuracy 0.9501\n",
            "Epoch 102, testing accuracy 0.9526\n",
            "Epoch 103, testing accuracy 0.953\n",
            "Epoch 104, testing accuracy 0.9527\n",
            "Epoch 105, testing accuracy 0.951\n",
            "Epoch 106, testing accuracy 0.953\n",
            "Epoch 107, testing accuracy 0.95\n",
            "Epoch 108, testing accuracy 0.9519\n",
            "Epoch 109, testing accuracy 0.954\n",
            "Epoch 110, testing accuracy 0.9529\n",
            "Epoch 111, testing accuracy 0.9492\n",
            "Epoch 112, testing accuracy 0.9528\n",
            "Epoch 113, testing accuracy 0.9503\n",
            "Epoch 114, testing accuracy 0.956\n",
            "Epoch 115, testing accuracy 0.9516\n",
            "Epoch 116, testing accuracy 0.9506\n",
            "Epoch 117, testing accuracy 0.9521\n",
            "Epoch 118, testing accuracy 0.9493\n",
            "Epoch 119, testing accuracy 0.9571\n",
            "Epoch 120, testing accuracy 0.953\n",
            "Epoch 121, testing accuracy 0.9427\n",
            "Epoch 122, testing accuracy 0.9525\n",
            "Epoch 123, testing accuracy 0.9541\n",
            "Epoch 124, testing accuracy 0.9532\n",
            "Epoch 125, testing accuracy 0.9548\n",
            "Epoch 126, testing accuracy 0.9546\n",
            "Epoch 127, testing accuracy 0.9556\n",
            "Epoch 128, testing accuracy 0.9517\n",
            "Epoch 129, testing accuracy 0.9543\n",
            "Epoch 130, testing accuracy 0.9504\n",
            "Epoch 131, testing accuracy 0.9545\n",
            "Epoch 132, testing accuracy 0.9554\n",
            "Epoch 133, testing accuracy 0.9529\n",
            "Epoch 134, testing accuracy 0.9511\n",
            "Epoch 135, testing accuracy 0.954\n",
            "Epoch 136, testing accuracy 0.9548\n",
            "Epoch 137, testing accuracy 0.9487\n",
            "Epoch 138, testing accuracy 0.9543\n",
            "Epoch 139, testing accuracy 0.9543\n",
            "Epoch 140, testing accuracy 0.952\n",
            "Epoch 141, testing accuracy 0.9538\n",
            "Epoch 142, testing accuracy 0.9495\n",
            "Epoch 143, testing accuracy 0.9528\n",
            "Epoch 144, testing accuracy 0.953\n",
            "Epoch 145, testing accuracy 0.9513\n",
            "Epoch 146, testing accuracy 0.955\n",
            "Epoch 147, testing accuracy 0.954\n",
            "Epoch 148, testing accuracy 0.9572\n",
            "Epoch 149, testing accuracy 0.9576\n",
            "Epoch 150, testing accuracy 0.9491\n",
            "Epoch 151, testing accuracy 0.9571\n",
            "Epoch 152, testing accuracy 0.9573\n",
            "Epoch 153, testing accuracy 0.9553\n",
            "Epoch 154, testing accuracy 0.9579\n",
            "Epoch 155, testing accuracy 0.9595\n",
            "Epoch 156, testing accuracy 0.9569\n",
            "Epoch 157, testing accuracy 0.9562\n",
            "Epoch 158, testing accuracy 0.9593\n",
            "Epoch 159, testing accuracy 0.9552\n",
            "Epoch 160, testing accuracy 0.9533\n",
            "Epoch 161, testing accuracy 0.9548\n",
            "Epoch 162, testing accuracy 0.9573\n",
            "Epoch 163, testing accuracy 0.9547\n",
            "Epoch 164, testing accuracy 0.959\n",
            "Epoch 165, testing accuracy 0.9593\n",
            "Epoch 166, testing accuracy 0.9579\n",
            "Epoch 167, testing accuracy 0.9583\n",
            "Epoch 168, testing accuracy 0.9542\n",
            "Epoch 169, testing accuracy 0.9599\n",
            "Epoch 170, testing accuracy 0.9542\n",
            "Epoch 171, testing accuracy 0.9578\n",
            "Epoch 172, testing accuracy 0.9544\n",
            "Epoch 173, testing accuracy 0.9504\n",
            "Epoch 174, testing accuracy 0.954\n",
            "Epoch 175, testing accuracy 0.9542\n",
            "Epoch 176, testing accuracy 0.958\n",
            "Epoch 177, testing accuracy 0.9541\n",
            "Epoch 178, testing accuracy 0.9595\n",
            "Epoch 179, testing accuracy 0.9522\n",
            "Epoch 180, testing accuracy 0.9554\n",
            "Epoch 181, testing accuracy 0.9529\n",
            "Epoch 182, testing accuracy 0.9556\n",
            "Epoch 183, testing accuracy 0.9565\n",
            "Epoch 184, testing accuracy 0.9586\n",
            "Epoch 185, testing accuracy 0.9606\n",
            "Epoch 186, testing accuracy 0.9548\n",
            "Epoch 187, testing accuracy 0.9584\n",
            "Epoch 188, testing accuracy 0.9549\n",
            "Epoch 189, testing accuracy 0.9569\n",
            "Epoch 190, testing accuracy 0.9609\n",
            "Epoch 191, testing accuracy 0.9609\n",
            "Epoch 192, testing accuracy 0.9601\n",
            "Epoch 193, testing accuracy 0.9586\n",
            "Epoch 194, testing accuracy 0.9608\n",
            "Epoch 195, testing accuracy 0.9608\n",
            "Epoch 196, testing accuracy 0.9562\n",
            "Epoch 197, testing accuracy 0.9608\n",
            "Epoch 198, testing accuracy 0.9591\n",
            "Epoch 199, testing accuracy 0.9596\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}